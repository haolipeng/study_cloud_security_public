# 数据存储与管理

> 网络安全流量分析系统的核心存储架构设计
> 技术栈：ClickHouse + Elasticsearch + MinIO + Redis

---

## 核心技术
- **分层存储**：根据数据类型和访问频率选择不同存储引擎
- **列式存储**：ClickHouse高性能OLAP存储
- **数据压缩**：减少存储空间，提升查询性能
- **生命周期管理**：自动归档和清理过期数据

---

## 存储架构设计

### 整体数据流
```
[原始PCAP] → MinIO (按需存储，深度取证)
    ↓
[基础Flow] → ClickHouse (全量存储)
    ↓
[协议元数据] → ClickHouse (HTTP/DNS/TLS日志)
    ↓
[流量特征] → ClickHouse (统计分析特征) ⭐ 核心
    ↓
[告警数据] → ClickHouse + Elasticsearch (告警管理)
```

### 数据分层模型
```
┌─────────────────────────────────────────────┐
│  Layer 1: 原始数据层 (PCAP)                 │
│  存储：MinIO/对象存储                        │
│  保留：7-30天（按需）                        │
│  用途：深度取证、流量重放                    │
└─────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────┐
│  Layer 2: 流量数据层 (NetFlow)              │
│  存储：ClickHouse                            │
│  保留：90天                                  │
│  用途：基础流信息、五元组查询                │
└─────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────┐
│  Layer 3: 协议元数据层                       │
│  存储：ClickHouse                            │
│  保留：60天                                  │
│  用途：HTTP/DNS/TLS详细信息                  │
└─────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────┐
│  Layer 4: 流量特征层 ⭐                      │
│  存储：ClickHouse                            │
│  保留：30天                                  │
│  用途：威胁检测、行为分析                    │
└─────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────┐
│  Layer 5: 告警数据层                         │
│  存储：ClickHouse + Elasticsearch            │
│  保留：180天                                 │
│  用途：告警管理、事件响应                    │
└─────────────────────────────────────────────┘
```

---

## 核心表结构设计

### 1. 基础流量表
```sql
-- NetFlow基础数据
CREATE TABLE flows (
    -- 时间字段
    timestamp DateTime,
    flow_start DateTime,
    flow_end DateTime,
    duration UInt32,  -- 秒
    
    -- 五元组
    src_ip IPv4,
    dst_ip IPv4,
    src_port UInt16,
    dst_port UInt16,
    protocol UInt8,
    
    -- 基础统计
    packets_total UInt64,
    bytes_total UInt64,
    packets_forward UInt32,
    packets_backward UInt32,
    bytes_forward UInt64,
    bytes_backward UInt64,
    
    -- TCP标志
    tcp_flags UInt8,
    syn_count UInt8,
    rst_count UInt8,
    fin_count UInt8,
    
    -- 应用层协议
    app_protocol String,  -- HTTP/DNS/TLS等
    
    -- 索引优化
    INDEX idx_src_ip src_ip TYPE bloom_filter GRANULARITY 1,
    INDEX idx_dst_ip dst_ip TYPE bloom_filter GRANULARITY 1,
    INDEX idx_protocol protocol TYPE set(0) GRANULARITY 1
    
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (timestamp, src_ip, dst_ip)
TTL timestamp + INTERVAL 90 DAY
SETTINGS index_granularity = 8192;
```

### 2. 流量特征表（⭐ 核心）
```sql
-- 流量统计特征表（用于威胁检测）
CREATE TABLE flow_features (
    -- 关联信息
    timestamp DateTime,
    flow_id String,
    src_ip IPv4,
    dst_ip IPv4,
    
    -- 基础特征
    packet_count UInt64,
    byte_count UInt64,
    duration Float32,
    flow_rate Float32,
    packet_rate Float32,
    
    -- 包长特征
    pkt_len_min UInt16,
    pkt_len_max UInt16,
    pkt_len_mean Float32,
    pkt_len_std Float32,
    fwd_pkt_len_mean Float32,
    bwd_pkt_len_mean Float32,
    
    -- 时间特征（IAT）
    iat_mean Float32,
    iat_std Float32,
    iat_min Float32,
    iat_max Float32,
    fwd_iat_mean Float32,
    bwd_iat_mean Float32,
    
    -- 活动/空闲特征
    active_mean Float32,
    active_std Float32,
    idle_mean Float32,
    idle_std Float32,
    
    -- 统计学特征
    entropy Float32,
    periodicity Float32,
    cv_packet_length Float32,
    cv_iat Float32,
    
    -- 行为特征
    upload_download_ratio Float32,
    unique_port_count UInt16,
    failed_connections UInt16,
    
    -- 标签（机器学习）
    is_anomaly UInt8,
    threat_type String,
    confidence Float32,
    
    INDEX idx_entropy entropy TYPE minmax GRANULARITY 1,
    INDEX idx_periodicity periodicity TYPE minmax GRANULARITY 1
    
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (timestamp, src_ip, dst_ip)
TTL timestamp + INTERVAL 30 DAY
SETTINGS index_granularity = 8192;
```

### 3. HTTP日志表
```sql
CREATE TABLE http_logs (
    timestamp DateTime,
    flow_id String,
    
    src_ip IPv4,
    dst_ip IPv4,
    src_port UInt16,
    dst_port UInt16,
    
    -- 请求信息
    method String,
    host String,
    uri String,
    uri_length UInt16,
    uri_depth UInt8,
    user_agent String,
    referer String,
    
    -- 响应信息
    status_code UInt16,
    content_type String,
    content_length UInt64,
    response_time Float32,
    
    -- 特征
    header_count UInt8,
    cookie_count UInt8,
    
    INDEX idx_host host TYPE bloom_filter GRANULARITY 1,
    INDEX idx_uri uri TYPE bloom_filter GRANULARITY 1,
    INDEX idx_status status_code TYPE set(0) GRANULARITY 1
    
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (timestamp, src_ip, host)
TTL timestamp + INTERVAL 60 DAY;
```

### 4. DNS日志表
```sql
CREATE TABLE dns_logs (
    timestamp DateTime,
    flow_id String,
    
    src_ip IPv4,
    dst_ip IPv4,
    
    -- 查询信息
    query_name String,
    query_type String,
    query_length UInt16,
    
    -- 域名特征（DGA检测）
    domain_entropy Float32,
    domain_length UInt16,
    subdomain_count UInt8,
    digit_ratio Float32,
    vowel_ratio Float32,
    
    -- 响应信息
    response_code String,
    answer_count UInt8,
    answers Array(String),
    ttl UInt32,
    
    -- 检测标签
    is_dga UInt8,
    
    INDEX idx_query query_name TYPE bloom_filter GRANULARITY 1,
    INDEX idx_entropy domain_entropy TYPE minmax GRANULARITY 1
    
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (timestamp, src_ip, query_name)
TTL timestamp + INTERVAL 60 DAY;
```

### 5. TLS日志表
```sql
CREATE TABLE tls_logs (
    timestamp DateTime,
    flow_id String,
    
    src_ip IPv4,
    dst_ip IPv4,
    
    -- TLS指纹
    ja3_fingerprint String,
    ja3s_fingerprint String,
    
    -- TLS信息
    tls_version String,
    cipher_suite String,
    sni String,
    
    -- 证书信息
    cert_issuer String,
    cert_subject String,
    cert_valid_days UInt16,
    cert_self_signed UInt8,
    
    -- 握手特征
    handshake_bytes UInt32,
    handshake_packets UInt8,
    
    INDEX idx_ja3 ja3_fingerprint TYPE bloom_filter GRANULARITY 1,
    INDEX idx_sni sni TYPE bloom_filter GRANULARITY 1
    
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (timestamp, src_ip, sni)
TTL timestamp + INTERVAL 60 DAY;
```

### 6. 告警表
```sql
CREATE TABLE alerts (
    -- 告警基本信息
    alert_id String,
    timestamp DateTime,
    severity Enum8('low' = 1, 'medium' = 2, 'high' = 3, 'critical' = 4),
    
    -- 关联信息
    flow_id String,
    rule_id String,
    rule_name String,
    
    -- 流量信息
    src_ip IPv4,
    dst_ip IPv4,
    src_port UInt16,
    dst_port UInt16,
    protocol String,
    
    -- 检测信息
    attack_type String,
    threat_category String,
    confidence Float32,
    
    -- 特征快照
    feature_snapshot String,  -- JSON格式
    
    -- 处理状态
    status Enum8('new' = 1, 'acknowledged' = 2, 'investigating' = 3, 'resolved' = 4, 'false_positive' = 5),
    assigned_to String,
    comments String,
    
    -- PCAP关联
    pcap_id String,
    
    INDEX idx_severity severity TYPE set(0) GRANULARITY 1,
    INDEX idx_status status TYPE set(0) GRANULARITY 1
    
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (timestamp, severity, src_ip)
TTL timestamp + INTERVAL 180 DAY;
```

---

## 数据写入模块

### 1. 数据写入管道
```go
package storage

type DataPipeline struct {
    clickhouse *ClickHouseClient
    buffer     *BatchBuffer
    flowCache  *FlowCache
}

type BatchBuffer struct {
    buffers  map[string][]interface{}
    maxSize  int
    mu       sync.Mutex
}

// 处理数据包
func (p *DataPipeline) ProcessPacket(packet *Packet) error {
    // 1. 更新Flow
    flow := p.flowCache.Update(packet)
    
    // 2. 协议解析
    metadata := p.parseProtocol(packet)
    
    // 3. Flow完成时写入
    if flow.IsComplete() {
        // 计算特征
        features := ExtractFeatures(flow)
        p.buffer.Add("flow_features", features)
        
        // 写入Flow记录
        p.buffer.Add("flows", flow.ToRecord())
        
        // 写入协议元数据
        if metadata != nil {
            p.buffer.Add(metadata.TableName(), metadata)
        }
        
        // 清理缓存
        p.flowCache.Remove(flow.ID)
    }
    
    // 4. 批量刷写
    if p.buffer.ShouldFlush() {
        return p.buffer.Flush(p.clickhouse)
    }
    
    return nil
}

// 批量缓冲
func (b *BatchBuffer) Add(table string, data interface{}) {
    b.mu.Lock()
    defer b.mu.Unlock()
    
    if b.buffers[table] == nil {
        b.buffers[table] = make([]interface{}, 0, 1000)
    }
    b.buffers[table] = append(b.buffers[table], data)
}

func (b *BatchBuffer) Flush(client *ClickHouseClient) error {
    b.mu.Lock()
    defer b.mu.Unlock()
    
    for table, data := range b.buffers {
        if len(data) == 0 {
            continue
        }
        
        // 批量插入
        if err := client.BatchInsert(table, data); err != nil {
            return err
        }
    }
    
    // 清空缓冲
    b.buffers = make(map[string][]interface{})
    return nil
}

func (b *BatchBuffer) ShouldFlush() bool {
    b.mu.Lock()
    defer b.mu.Unlock()
    
    totalSize := 0
    for _, data := range b.buffers {
        totalSize += len(data)
    }
    return totalSize >= b.maxSize
}
```

### 2. ClickHouse客户端
```go
package storage

type ClickHouseClient struct {
    conn *sql.DB
}

func NewClickHouseClient(dsn string) (*ClickHouseClient, error)

// 批量插入
func (c *ClickHouseClient) BatchInsert(table string, data []interface{}) error

// 单次插入（小批量）
func (c *ClickHouseClient) Insert(table string, data interface{}) error

// 查询
func (c *ClickHouseClient) Query(query string, args ...interface{}) (*sql.Rows, error)

// 执行DDL
func (c *ClickHouseClient) Execute(query string) error
```

### 3. Flow缓存管理
```go
package storage

type FlowCache struct {
    flows    map[string]*Flow
    timeout  time.Duration
    mu       sync.RWMutex
}

func NewFlowCache(timeout time.Duration) *FlowCache

func (c *FlowCache) Update(packet *Packet) *Flow
func (c *FlowCache) Get(flowID string) (*Flow, bool)
func (c *FlowCache) Remove(flowID string)
func (c *FlowCache) CleanExpired()
```

---

## 数据查询模块

### 1. 查询服务
```go
package query

type QueryService struct {
    clickhouse *ClickHouseClient
    cache      *QueryCache
}

// 流量特征查询
func (s *QueryService) QueryFlowFeatures(req *FeatureQueryRequest) ([]*FlowFeatures, error)

// 协议日志查询
func (s *QueryService) QueryHTTPLogs(req *HTTPQueryRequest) ([]*HTTPLog, error)
func (s *QueryService) QueryDNSLogs(req *DNSQueryRequest) ([]*DNSLog, error)
func (s *QueryService) QueryTLSLogs(req *TLSQueryRequest) ([]*TLSLog, error)

// 基础Flow查询
func (s *QueryService) QueryFlows(req *FlowQueryRequest) ([]*Flow, error)

// 告警查询
func (s *QueryService) QueryAlerts(req *AlertQueryRequest) ([]*Alert, error)
```

### 2. 统计聚合查询
```go
package query

// Top N查询
func (s *QueryService) GetTopTalkers(timeRange TimeRange, limit int) ([]*TopTalker, error)
func (s *QueryService) GetTopPorts(timeRange TimeRange, limit int) ([]*PortStat, error)
func (s *QueryService) GetTopProtocols(timeRange TimeRange) (map[string]int64, error)

// 趋势查询
func (s *QueryService) GetTrafficTrend(timeRange TimeRange, interval string) ([]*TrendPoint, error)
func (s *QueryService) GetAlertTrend(timeRange TimeRange, interval string) ([]*TrendPoint, error)

// 统计查询
func (s *QueryService) GetTrafficStatistics(timeRange TimeRange) (*Statistics, error)
```

### 3. 威胁狩猎查询
```go
package query

// C2 Beacon检测
func (s *QueryService) HuntC2Beacon(timeRange TimeRange) ([]*Suspicious, error) {
    query := `
        SELECT 
            ff.timestamp, ff.src_ip, ff.dst_ip,
            ff.periodicity, ff.iat_mean, ff.iat_std, ff.entropy
        FROM flow_features ff
        WHERE ff.timestamp BETWEEN ? AND ?
            AND ff.periodicity > 0.8
            AND ff.iat_std < 1.0
            AND ff.entropy > 7.0
        ORDER BY ff.periodicity DESC
        LIMIT 100
    `
    // 执行查询...
}

// DGA域名检测
func (s *QueryService) HuntDGA(timeRange TimeRange) ([]*SuspiciousDNS, error) {
    query := `
        SELECT *
        FROM dns_logs
        WHERE timestamp BETWEEN ? AND ?
            AND domain_entropy > 4.0
            AND vowel_ratio < 0.3
        ORDER BY domain_entropy DESC
        LIMIT 100
    `
    // 执行查询...
}

// 数据泄露检测
func (s *QueryService) HuntDataExfiltration(timeRange TimeRange) ([]*Suspicious, error) {
    query := `
        SELECT 
            ff.timestamp, ff.src_ip, ff.dst_ip,
            ff.upload_download_ratio, ff.byte_count
        FROM flow_features ff
        WHERE ff.timestamp BETWEEN ? AND ?
            AND ff.upload_download_ratio > 2.0
            AND ff.byte_count > 1000000
        ORDER BY ff.upload_download_ratio DESC
        LIMIT 100
    `
    // 执行查询...
}
```

### 4. 查询缓存
```go
package query

type QueryCache struct {
    redis *redis.Client
    ttl   time.Duration
}

func (c *QueryCache) Get(key string) (interface{}, bool)
func (c *QueryCache) Set(key string, value interface{}) error
func (c *QueryCache) Delete(key string) error
func (c *QueryCache) Clear() error
```

---

## 数据生命周期管理

### 1. 生命周期策略
```go
package storage

type LifecycleManager struct {
    clickhouse *ClickHouseClient
    minio      *MinIOClient
}

type DataTier struct {
    TableName  string
    HotDays    int  // 热数据期（天）
    WarmDays   int  // 温数据期（天）
    ColdDays   int  // 冷数据期（天）
    TTL        int  // 总保留期（天）
}

var DataTiers = []DataTier{
    {TableName: "flows", HotDays: 7, WarmDays: 30, ColdDays: 90, TTL: 90},
    {TableName: "flow_features", HotDays: 7, WarmDays: 30, TTL: 30},
    {TableName: "http_logs", HotDays: 7, WarmDays: 30, ColdDays: 60, TTL: 60},
    {TableName: "dns_logs", HotDays: 7, WarmDays: 30, ColdDays: 60, TTL: 60},
    {TableName: "tls_logs", HotDays: 7, WarmDays: 30, ColdDays: 60, TTL: 60},
    {TableName: "alerts", HotDays: 30, WarmDays: 90, ColdDays: 180, TTL: 180},
}

// 归档冷数据
func (m *LifecycleManager) ArchiveColdData(tier DataTier) error

// 删除过期数据
func (m *LifecycleManager) DeleteExpiredData(tier DataTier) error

// 数据压缩
func (m *LifecycleManager) CompressOldData(tier DataTier) error

// 定期清理任务
func (m *LifecycleManager) RunCleanupTask()
```

### 2. PCAP管理
```go
package storage

type PCAPManager struct {
    minio  *MinIOClient
    index  *PCAPIndexer
}

type PCAPMetadata struct {
    ID          string
    StartTime   time.Time
    EndTime     time.Time
    PacketCount int64
    ByteCount   int64
    StoragePath string
    FlowIDs     []string
}

func (m *PCAPManager) SavePCAP(data []byte, metadata *PCAPMetadata) error
func (m *PCAPManager) GetPCAP(pcapID string) ([]byte, error)
func (m *PCAPManager) DeletePCAP(pcapID string) error
func (m *PCAPManager) SearchPCAP(filters *PCAPFilters) ([]*PCAPMetadata, error)
func (m *PCAPManager) RotateOldPCAPs(olderThan time.Time) error
```

---

## 性能优化

### 1. 物化视图（预聚合）
```sql
-- 每小时流量统计
CREATE MATERIALIZED VIEW flows_hourly
ENGINE = SummingMergeTree()
PARTITION BY toYYYYMM(hour)
ORDER BY (hour, src_ip)
AS SELECT
    toStartOfHour(timestamp) as hour,
    src_ip,
    dst_ip,
    protocol,
    COUNT(*) as flow_count,
    SUM(bytes_total) as total_bytes,
    SUM(packets_total) as total_packets
FROM flows
GROUP BY hour, src_ip, dst_ip, protocol;

-- 每日告警统计
CREATE MATERIALIZED VIEW alerts_daily
ENGINE = SummingMergeTree()
PARTITION BY toYYYYMM(day)
ORDER BY (day, severity)
AS SELECT
    toDate(timestamp) as day,
    severity,
    attack_type,
    COUNT(*) as alert_count
FROM alerts
GROUP BY day, severity, attack_type;
```

### 2. 分区优化
```go
package storage

type PartitionManager struct {
    clickhouse *ClickHouseClient
}

// 创建新分区
func (m *PartitionManager) CreatePartition(table string, date time.Time) error

// 删除旧分区
func (m *PartitionManager) DropPartition(table string, date time.Time) error

// 合并小分区
func (m *PartitionManager) OptimizePartition(table string, date time.Time) error
```

### 3. 查询优化
```go
package query

type QueryOptimizer struct {
    queryCache *QueryCache
}

// 查询计划分析
func (o *QueryOptimizer) AnalyzeQuery(query string) *QueryPlan

// 查询重写
func (o *QueryOptimizer) RewriteQuery(query string) string

// 并行查询
func (o *QueryOptimizer) ParallelQuery(queries []string) ([]interface{}, error)
```

---

## 存储容量规划

### 数据量估算（1Gbps网络环境）

| 数据类型 | 原始大小/天 | 压缩后/天 | 保留期 | 总容量 |
|---------|------------|----------|--------|--------|
| PCAP | 10TB | 3TB | 7天 | 21TB |
| Flows | 100GB | 10GB | 90天 | 900GB |
| Flow Features | 50GB | 5GB | 30天 | 150GB |
| HTTP Logs | 150GB | 15GB | 60天 | 900GB |
| DNS Logs | 30GB | 3GB | 60天 | 180GB |
| TLS Logs | 20GB | 2GB | 60天 | 120GB |
| Alerts | 5GB | 500MB | 180天 | 90GB |
| **总计** | **~10.4TB/天** | **~1TB/天** | - | **~23TB** |

### 硬件配置建议

#### 中小规模（< 1Gbps）
```
- 存储：10TB SSD（热数据） + 50TB HDD（冷数据）
- 内存：64GB
- CPU：16核
```

#### 大规模（1-10Gbps）
```
- 存储：50TB SSD（热数据） + 200TB HDD（冷数据）
- 内存：256GB
- CPU：64核
- 节点：ClickHouse 3节点集群
```

---

## 工程化要点
- 批量写入优化（5000-10000条/批）
- 查询缓存机制（Redis）
- 分区自动管理（按月分区）
- 数据压缩算法（LZ4/ZSTD）
- 冷热数据分离（SSD + HDD）
- 索引策略优化（bloom_filter/minmax）

